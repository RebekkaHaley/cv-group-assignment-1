{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import io # Input/Output Module\n",
    "import os # OS interfaces\n",
    "import cv2 # OpenCV package\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from urllib import request # module for opening HTTP requests\n",
    "from matplotlib import pyplot as plt # Plotting library\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%; height:140px\">\n",
    "    <img src=\"https://www.kuleuven.be/iportfolio/images/logos/kuleuven.png\" width = 300px, heigh = auto align=left>\n",
    "</div>\n",
    "\n",
    "\n",
    "KUL H02A5a Computer Vision: Group Assignment 1\n",
    "Student numbers: <span style=\"color:red\">r0788301, r1031990, r0637325, s0087053, r0121724</span>.<br>\n",
    "Student names: <span style=\"color:blue\">Nele Gorissen, Rebekka Haley, Ward Janssens, Frederik Bayart, Koen Van Leuvenhaege</span>.<br><br>\n",
    "\n",
    "The goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n",
    "\n",
    "In this assignment you are a group of computer vision experts that have been invited to ECCV 2025 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n",
    "\n",
    "---------------------------------------------------------------\n",
    "This notebook is structured as follows:\n",
    "0. Data loading & Preprocessing\n",
    "1. Feature Representations\n",
    "2. Evaluation Metrics \n",
    "3. Classifiers\n",
    "4. Experiments\n",
    "5. Publishing best results\n",
    "6. Discussion\n",
    "\n",
    "Make sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n",
    "\n",
    "Fill in your student numbers above and get to it! Good luck! \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> This notebook is just a example/template, feel free to adjust in any way you please! Just keep things organised and document accordingly!\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> Clearly indicate the improvements that you make!!! You can for instance use titles like: <i>3.1. Improvement: Non-linear SVM with RBF Kernel.<i>\n",
    "</div>\n",
    "    \n",
    "---------------------------------------------------------------\n",
    "# 0. Data loading & Preprocessing\n",
    "\n",
    "## 0.1. Loading data\n",
    "The training set is many times smaller than the test set and this might strike you as odd, however, this is close to a real world scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the \"input/\" folder\n",
    "# If you run locally with your input files at another location, place change the value of the \"input_folder\" variable to point to your folder\n",
    "input_folder = 'data'\n",
    "\n",
    "train = pd.read_csv(\n",
    "    f'{input_folder}/train_set.csv', index_col=0)\n",
    "train.index = train.index.rename('id')\n",
    "\n",
    "test = pd.read_csv(\n",
    "    f'{input_folder}/test_set.csv', index_col=0)\n",
    "test.index = test.index.rename('id')\n",
    "\n",
    "# read the images as numpy arrays and store in \"img\" column\n",
    "train['img'] = [\n",
    "    cv2.cvtColor(np.load(f'{input_folder}/train/train_{index}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB)\n",
    "    for index, row in train.iterrows()]\n",
    "\n",
    "test['img'] = [\n",
    "    cv2.cvtColor(np.load(f'{input_folder}/test/test_{index}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB)\n",
    "    for index, row in test.iterrows()]\n",
    "\n",
    "train_size, test_size = len(train),len(test)\n",
    "\n",
    "\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/).\n",
    "\n",
    "## 0.2. A first look\n",
    "Let's have a look at the data columns and class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training set contains an identifier, name, image information and class label\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test set only contains an identifier and corresponding image information.\n",
    "\n",
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class distribution in the training set:\n",
    "train.groupby('name').agg({'img':'count', 'class': 'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **Jesse is assigned the classification label 1**, and **Mila is assigned the classification label 2**. The dataset also contains 20 images of **look alikes (assigned classification label 0)** and the raw images. \n",
    "\n",
    "## 0.3. Preprocess data\n",
    "### 0.3.1 Example: HAAR face detector\n",
    "In this example we use the [HAAR feature based cascade classifiers](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html) to detect faces, then the faces are resized so that they all have the same shape. If there are multiple faces in an image, we only take the first one. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You can write temporary files to <code>/kaggle/temp/</code> or <code>../../tmp</code>, but they won't be saved outside of the current session\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAARPreprocessor():\n",
    "    \"\"\"Preprocessing pipeline built around HAAR feature based cascade classifiers. \"\"\"\n",
    "    \n",
    "    def __init__(self, path, face_size):\n",
    "        self.face_size = face_size\n",
    "        file_path = os.path.join(path, \"haarcascade_frontalface_default.xml\")\n",
    "        if not os.path.exists(file_path): \n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            self.download_model(file_path)\n",
    "        \n",
    "        self.classifier = cv2.CascadeClassifier(file_path)\n",
    "  \n",
    "    def download_model(self, path):\n",
    "        url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/\"\\\n",
    "            \"haarcascades/haarcascade_frontalface_default.xml\"\n",
    "        \n",
    "        with request.urlopen(url) as r, open(path, 'wb') as f:\n",
    "            f.write(r.read())\n",
    "            \n",
    "    def detect_faces(self, img):\n",
    "        \"\"\"Detect all faces in an image.\"\"\"\n",
    "        \n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        return self.classifier.detectMultiScale(\n",
    "            img_gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        \n",
    "    def extract_faces(self, img):\n",
    "        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n",
    "        \n",
    "        faces = self.detect_faces(img)\n",
    "\n",
    "        return [img[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
    "    \n",
    "    def preprocess(self, data_row):\n",
    "        faces = self.extract_faces(data_row['img'])\n",
    "        \n",
    "        # if no faces were found, return None\n",
    "        if len(faces) == 0:\n",
    "            nan_img = np.empty(self.face_size + (3,))\n",
    "            nan_img[:] = np.nan\n",
    "            return nan_img\n",
    "        \n",
    "        # only return the first face\n",
    "        return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "    def __call__(self, data):\n",
    "        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### chatgpt ###\n",
    "def KernelPCARun(X):\n",
    "    # Using RBF kernel PCA\n",
    "    kpca = KernelPCA(n_components=100, kernel='rbf', gamma=1e-5, fit_inverse_transform=False)\n",
    "    X_kpca = kpca.fit_transform(X)  # shape: (n_samples, 100)\n",
    "\n",
    "    # We chose 100 components arbitrarily here; one might tune gamma and n_components.\n",
    "    print(\"Kernel PCA output shape:\", X_kpca.shape)\n",
    "    return X_kpca\n",
    "\n",
    "def RunPCA(X,threshold = 0.95):\n",
    "    pca = PCA(n_components=threshold, svd_solver='full')  # keep 95% of variance\n",
    "    X_pca = pca.fit_transform(X)  # shape: (n_samples, k), k chosen by 95% var criterion\n",
    "\n",
    "    print(\"Original dimension:\", X.shape[1])\n",
    "    print(\"Reduced dimension:\", X_pca.shape[1])\n",
    "    # explained_variance_ratio_ tells us how much variance each PC explains\n",
    "    cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "    print(\"Cumulative explained variance:\", cum_var)\n",
    "    return pca,X_pca\n",
    "\n",
    "def vizualiseEigenFaces(pca):\n",
    "    eigenfaces = pca.components_[:16]  # top 16 eigenfaces\n",
    "    h = w = 100  # if images were 100x100 grayscale (D=10000); adjust for actual size\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(6,6))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(eigenfaces):\n",
    "            # reshape eigenface into image and show\n",
    "            ax.imshow(eigenfaces[i].reshape(h, w, -1))  # if color, remove cmap\n",
    "            ax.set_title(f\"PC {i+1}\")\n",
    "            ax.axis('off')\n",
    "    plt.suptitle(\"Top 16 Eigenfaces\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualise**\n",
    "\n",
    "Let's plot a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter to play with \n",
    "FACE_SIZE = (100, 100)\n",
    "\n",
    "def plot_image_sequence(data, n, imgs_per_row=7):\n",
    "    n_rows = 1 + int(n/(imgs_per_row+1))\n",
    "    n_cols = min(imgs_per_row, n)\n",
    "\n",
    "    f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n",
    "    if n_rows > 1 or n_cols > 1:\n",
    "        axes = ax.flatten()\n",
    "    else:\n",
    "        axes = [ax]\n",
    "\n",
    "\n",
    "    for i in range(n):\n",
    "        if n == 1:\n",
    "            ax.imshow(data[i])\n",
    "        elif n_rows > 1:\n",
    "            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i])\n",
    "        else:\n",
    "            ax[int(i%n)].imshow(data[i])\n",
    "        axes[i].set_xlabel(f\"Index: {i}\", fontsize=60)\n",
    "    plt.show()\n",
    "\n",
    "#preprocessed data \n",
    "preprocessor = HAARPreprocessor(path = '../../tmp', face_size=FACE_SIZE)\n",
    "\n",
    "train_X, train_y = preprocessor(train), train['class'].values\n",
    "test_X = preprocessor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code further preprocssing\n",
    "# manual removal of outliers\n",
    "IndicesToRemove1 = [0,6,15,12,18] #ind 19,18,26?\n",
    "IndicesToRemove2 = [5,7,24] # 14,15?\n",
    "train_X_1        = np.delete(train_X[train_y == 1], IndicesToRemove1, axis=0)\n",
    "train_X_2        = np.delete(train_X[train_y == 2], IndicesToRemove2, axis=0)\n",
    "train_X_removed  = np.concatenate((train_X_1, train_X_2, train_X[train_y == 0]))\n",
    "\n",
    "train_Y_1 = np.delete(train_y[train_y == 1], IndicesToRemove1, axis=0)\n",
    "train_Y_2 = np.delete(train_y[train_y == 2], IndicesToRemove2, axis=0)\n",
    "train_Y_removed  = np.concatenate((train_Y_1, train_Y_2, train_y[train_y == 0]))\n",
    "\n",
    "#code to visualize the removal\n",
    "plot_image_sequence(train_X_removed[train_Y_removed == 1], n=25, imgs_per_row=10)\n",
    "plot_image_sequence(train_X[train_y == 1], n=30, imgs_per_row=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_Y_removed\n",
    "train_X_removed_flat = train_X_removed.reshape(train_X_removed.shape[0], -1)\n",
    "\n",
    "# normalization\n",
    "pca,X_pca = RunPCA(train_X_removed_flat,threshold = 0.95)\n",
    "Kpca      = KernelPCARun(train_X_removed_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Michael and Sarah\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 0], n=20, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Jesse\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 1], n=30, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces of Mila\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 2], n=30, imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4. Store Preprocessed data (optional)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\". Feel free to use this to store intermediary results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data - ATTENTION: set variable \"must_save_prepped_data\" to True if you really want to save the data\n",
    "prep_path = 'prepped_data/'\n",
    "must_save_prepped_data = False\n",
    "if must_save_prepped_data:\n",
    "    if not os.path.exists(prep_path):\n",
    "        os.mkdir(prep_path)\n",
    "        \n",
    "    np.save(os.path.join(prep_path, 'train_X.npy'), train_X)\n",
    "    np.save(os.path.join(prep_path, 'train_y.npy'), train_y)\n",
    "    np.save(os.path.join(prep_path, 'test_X.npy'), test_X)\n",
    "\n",
    "# load preprocessed data if it exists (check if the folder and at least one of the needed files exist)\n",
    "if os.path.exists(prep_path) and os.path.exists(os.path.join(prep_path, 'train_X.npy')):\n",
    "    train_X = np.load(os.path.join(prep_path, 'train_X.npy'))\n",
    "    train_y = np.load(os.path.join(prep_path, 'train_y.npy'))\n",
    "    test_X = np.load(os.path.join(prep_path, 'test_X.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to rock!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Representations\n",
    "## 1.0. Example: Identify feature extractor\n",
    "Our example feature extractor doesn't actually do anything... It just returns the input:\n",
    "$$\n",
    "\\forall x : f(x) = x.\n",
    "$$\n",
    "\n",
    "It does make for a good placeholder and baseclass ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityFeatureExtractor:\n",
    "    \"\"\"A simple function that returns the input\"\"\"\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Baseline 1: HOG feature extractor/Scale Invariant Feature Transform\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if an image is gray_scale: color image shave shape length 3, gray_scale have shape length 2\n",
    "def is_gray_scale(image):\n",
    "    return len(image.shape) == 2\n",
    "\n",
    "class HOGFeatureExtractor(IdentityFeatureExtractor):\n",
    "\n",
    "    def __init__(self, **params):\n",
    "        self.params = params\n",
    "        self.hogDescriptor = cv2.HOGDescriptor(_winSize = (96, 96),\n",
    "                                               _blockSize = (16, 16),    # Block size for normalization\n",
    "                                               _blockStride = (8, 8),    # How far the block moves in each step\n",
    "                                               _cellSize = (8, 8),       # Size of the cells in which gradients will be computed\n",
    "                                               _nbins = 9)               # Number of orientation bins\n",
    "\n",
    "    def transform(self, image):\n",
    "        # make sure the image is in the correct format\n",
    "        image = image.astype(np.uint8)\n",
    "        # convert the image to gray_scale if not yet done\n",
    "        if not is_gray_scale(image):\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # as the input images are 100x100, remove the outer 2 pixels to let it\n",
    "        # be an acceptable size (96x96 - multiple of 16) for HOG processing\n",
    "        image = image[2:-2, 2:-2] # reduce to 96x96\n",
    "        # extract the HOG features\n",
    "        hog_features = self.hogDescriptor.compute(image)\n",
    "        # make it a flat list for further processing\n",
    "        hog_features_flat = hog_features.flatten()\n",
    "        return hog_features_flat\n",
    "\n",
    "hog_features_list = []\n",
    "hfe = HOGFeatureExtractor()\n",
    "for image in train_X:\n",
    "    result = hfe.transform(image)\n",
    "    hog_features_list.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. t-SNE Plots\n",
    "The code below is for a big part taken over from the tutorial https://learnopencv.com/t-sne-for-feature-visualization/, whch explains how to construct t-SNE plots.  Extra input was taken from https://static.ux5.de/Moving-Object-Detection-with-OpenCV/archiv/learnopencv-master/TSNE/tsne.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for the dots in the tSNE graph and for the rectangles around the images\n",
    "colors_per_class = {\n",
    "    0: [0, 0, 0],\n",
    "    1: [255, 107, 107],\n",
    "    2: [100, 100, 255]\n",
    "}\n",
    "# Names for the classes to populate the legend\n",
    "names_per_class = {\n",
    "    0: \"Dunno\",\n",
    "    1: \"Jesse Eisenberg\",\n",
    "    2: \"Mila Kunas\"\n",
    "}\n",
    "\n",
    "#\n",
    "# Class to visualize features and images the features represent\n",
    "#\n",
    "class TsneVisualiser:\n",
    "    def __init__(self):\n",
    "        self.tsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3)\n",
    "\n",
    "    # normalize the values to show them in a 0-1 plot\n",
    "    def normalize(self, x):\n",
    "        # compute the distribution range\n",
    "        value_range = (np.max(x) - np.min(x))\n",
    "\n",
    "        # move the distribution so that it starts from zero\n",
    "        # by extracting the minimal value from all its values\n",
    "        starts_from_zero = x - np.min(x)\n",
    "\n",
    "        # make the distribution fit [0; 1] by dividing by its range\n",
    "        return starts_from_zero / value_range\n",
    "\n",
    "    # Perform the actual t-SNE feature transformation\n",
    "    def extract_features_embedded(self, features):\n",
    "        features_embedded = self.tsne.fit_transform(features)\n",
    "\n",
    "        # extract x and y coordinates representing the positions of the images on T-SNE plot\n",
    "        tx = features_embedded[:, 0]\n",
    "        ty = features_embedded[:, 1]\n",
    "\n",
    "        tx = self.normalize(tx)\n",
    "        ty = self.normalize(ty)\n",
    "        return tx, ty\n",
    "\n",
    "    def visualise_features(self, features, labels):\n",
    "        tx, ty = self.extract_features_embedded(features)\n",
    "\n",
    "        # initialize a matplotlib plot\n",
    "        fig = plt.figure(\"Jesse or Mila features\")\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "        # for every class, we'll add a scatter plot separately\n",
    "        for label in colors_per_class:\n",
    "            # find the samples of the current class in the data\n",
    "            current_label_indices = [i for i, l in enumerate(labels) if l == label]\n",
    "\n",
    "            # extract the x and y coordinates of the points of this class only\n",
    "            current_tx = np.take(tx, current_label_indices)\n",
    "            current_ty = np.take(ty, current_label_indices)\n",
    "\n",
    "            # convert the class color to matplotlib format\n",
    "            color = np.array(colors_per_class[label], dtype=np.float32) / 255\n",
    "\n",
    "            # add a scatter plot with the corresponding color and label\n",
    "            ax.scatter(current_tx, current_ty, c=color, label=names_per_class[label])\n",
    "\n",
    "        # show the legend\n",
    "        ax.legend()\n",
    "\n",
    "        # don't block when showing the plot\n",
    "        #plt.show(block=False)\n",
    "\n",
    "    # Compute the coordinates of the image on the plot\n",
    "    def compute_plot_coordinates(self, image, x, y, image_centers_area_size, offset):\n",
    "        image_height, image_width, _ = image.shape\n",
    "\n",
    "        # compute the image center coordinates on the plot\n",
    "        center_x = int(image_centers_area_size * x) + offset\n",
    "\n",
    "        # in matplotlib, the y axis is directed upward\n",
    "        # to have the same here, we need to mirror the y coordinate\n",
    "        center_y = int(image_centers_area_size * (1 - y)) + offset\n",
    "\n",
    "        # knowing the image center,\n",
    "        # compute the coordinates of the top left and bottom right corner\n",
    "        tl_x = center_x - int(image_width / 2)\n",
    "        tl_y = center_y - int(image_height / 2)\n",
    "\n",
    "        br_x = tl_x + image_width\n",
    "        br_y = tl_y + image_height\n",
    "\n",
    "        return tl_x, tl_y, br_x, br_y\n",
    "\n",
    "    def scale_image(self, image, max_image_size):\n",
    "        try:\n",
    "            image = np.array(image, dtype=np.uint8)\n",
    "            return cv2.resize(image, None, fx=(max_image_size/image.shape[0]), fy=(max_image_size/image.shape[1]))\n",
    "        except:\n",
    "            print(f\"scale_image failed for image {image.shape} and max size {max_image_size}\")\n",
    "            return image\n",
    "\n",
    "    def draw_rectangle_by_class(self, image, label):\n",
    "        image_height, image_width, _ = image.shape\n",
    "\n",
    "        # get the color corresponding to image class\n",
    "        color = colors_per_class[label]\n",
    "        image_with_rect = cv2.rectangle(image, (0, 0), (image_width - 1, image_height - 1), color, thickness=5)\n",
    "\n",
    "        return image_with_rect\n",
    "\n",
    "    def visualise_images(self, images, labels, tx, ty):\n",
    "        # init the plot as white canvas\n",
    "        plot_size = 1000 # plot size\n",
    "        tsne_plot = 255 * np.ones((plot_size, plot_size, 3), np.uint8)\n",
    "        max_image_size = 100\n",
    "        # we'll put the image centers in the central area of the plot\n",
    "        # and use offsets to make sure the images fit the plot\n",
    "        offset = max_image_size // 2\n",
    "        image_centers_area_size = plot_size - 2 * offset\n",
    "\n",
    "        # now we'll put a small copy of every image to its corresponding T-SNE coordinate\n",
    "        for image, label, x, y in zip(images, labels, tx, ty):\n",
    "            # image = cv2.imread(image_path)\n",
    "\n",
    "            # scale the image to put it to the plot\n",
    "            image = self.scale_image(image, max_image_size)\n",
    "\n",
    "            # draw a rectangle with a color corresponding to the image class\n",
    "            image = self.draw_rectangle_by_class(image, label)\n",
    "\n",
    "            # compute the coordinates of the image on the scaled plot visualization\n",
    "            tl_x, tl_y, br_x, br_y = self.compute_plot_coordinates(image, x, y, image_centers_area_size, offset)\n",
    "\n",
    "            # put the image to its t-SNE coordinates using numpy sub-array indices\n",
    "            tsne_plot[tl_y:br_y, tl_x:br_x, :] = image\n",
    "\n",
    "        image_rgb = cv2.cvtColor(tsne_plot, cv2.COLOR_BGR2RGB)\n",
    "        # initialize a matplotlib plot\n",
    "        fig = plt.figure(\"Jesse or Mila images\")\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "        plt.imshow(image_rgb)\n",
    "        #plt.axis(\"off\")\n",
    "\n",
    "tv = TsneVisualiser()\n",
    "tv.visualise_features(np.array(hog_features_list), train_y)\n",
    "tx, ty = tv.extract_features_embedded(np.array(hog_features_list))\n",
    "tv.visualise_images(train_X, train_y, tx, ty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Discussion\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Baseline 2: PCA feature extractor\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAFeatureExtractor(IdentityFeatureExtractor):\n",
    "    \"\"\"TODO: this feature extractor is under construction\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def transform(self, X):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def inverse_transform(self, X):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Eigenface Plots\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Feature Space Plots\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Discussion\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation Metrics\n",
    "## 2.0. Example: Accuracy\n",
    "As example metric we take the accuracy. Informally, accuracy is the proportion of correct predictions over the total amount of predictions. It is used a lot in classification but it certainly has its disadvantages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classifiers\n",
    "## 3.0. Example: The *'not so smart'* classifier\n",
    "This random classifier is not very complicated. It makes predictions at random, based on the distribution obseved in the training set. **It thus assumes** that the class labels of the test set will be distributed similarly to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomClassificationModel:\n",
    "    \"\"\"Random classifier, draws a random sample based on class distribution observed \n",
    "    during training.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Adjusts the class ratio instance variable to the one observed in y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : tensor\n",
    "            Training set\n",
    "        y : array\n",
    "            Training set labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : RandomClassificationModel\n",
    "        \"\"\"\n",
    "        \n",
    "        self.classes, self.class_ratio = np.unique(y, return_counts=True)\n",
    "        self.class_ratio = self.class_ratio / self.class_ratio.sum()\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Samples labels for the input data. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : tensor\n",
    "            dataset\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_star : array\n",
    "            'Predicted' labels\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(0)\n",
    "        return np.random.choice(self.classes, size = X.shape[0], p=self.class_ratio)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Baseline 1: My favorite classifier\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_Classifier():\n",
    "    def __init__(self):\n",
    "        self.hfe = HOGFeatureExtractor()\n",
    "        self.svc = SVC(kernel='linear', probability=True)  # Linear kernel for good performance\n",
    "        self.is_trained = False\n",
    "\n",
    "    def train_with_images(self, train_x_images, train_y):\n",
    "        hog_features_list = []\n",
    "        hfe = HOGFeatureExtractor()\n",
    "        for image in train_x_images:\n",
    "            result = hfe.transform(image)\n",
    "            hog_features_list.append(result)\n",
    "        self.train_with_hog_features(hog_features_list, train_y)\n",
    "\n",
    "    def train_with_hog_features(self, hog_features_list, train_y):\n",
    "        self.svc.fit(hog_features_list, train_y)\n",
    "        self.is_trained = True\n",
    "\n",
    "    def predict(self, images):\n",
    "        if self.is_trained:\n",
    "            hog_features_list = []\n",
    "            for image in images:\n",
    "                result = self.hfe.transform(image)\n",
    "                hog_features_list.append(result)\n",
    "            y_predict = self.svc.predict(hog_features_list)\n",
    "            return y_predict\n",
    "        else: # not trained!!!\n",
    "            print(f\"{self.__class__.__name__}: not trained!\")\n",
    "\n",
    "def plot_image_sequence(data, n, imgs_per_row=7):\n",
    "    n_rows = 1 + int(n/(imgs_per_row+1))\n",
    "    n_cols = min(imgs_per_row, n)\n",
    "\n",
    "    f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n",
    "    for i in range(n):\n",
    "        if n == 1:\n",
    "            ax.imshow(data[i])\n",
    "        elif n_rows > 1:\n",
    "            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i])\n",
    "        else:\n",
    "            ax[int(i%n)].imshow(data[i])\n",
    "    plt.show()\n",
    "\n",
    "svm = SVM_Classifier()\n",
    "svm.train_with_hog_features(hog_features_list, train_y)\n",
    "\n",
    "y_predict = svm.predict(test_X)\n",
    "#    y_predict = svm.predict(test_features_list)\n",
    "print(len(y_predict))\n",
    "\n",
    "jesse_eisenberg_predict = []\n",
    "mila_kunas_predict = []\n",
    "dunno_predict = []\n",
    "\n",
    "for i in range(len(y_predict)):\n",
    "    if y_predict[i] == 1:\n",
    "        jesse_eisenberg_predict.append(test_X[i])\n",
    "    elif y_predict[i] == 2:\n",
    "        mila_kunas_predict.append(test_X[i])\n",
    "    else:\n",
    "        dunno_predict.append(test_X[i])\n",
    "\n",
    "my_favourite_predict = jesse_eisenberg_predict\n",
    "plot_image_sequence(my_favourite_predict, int(np.min([25, len(my_favourite_predict)])), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Experiments\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> Do <i>NOT</i> use this section to keep track of every little change you make in your code! Instead, highlight the most important findings and the major (best) pipelines that you've discovered.  \n",
    "</div>\n",
    "<br>\n",
    "\n",
    "## 4.0. Example: basic pipeline\n",
    "The basic pipeline takes any input and samples a label based on the class label distribution of the training set. As expected the performance is very poor, predicting approximately 1/4 correctly on the training set. There is a lot of room for improvement but this is left to you ;). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = IdentityFeatureExtractor() \n",
    "classifier = RandomClassificationModel()\n",
    "\n",
    "# train the model on the features\n",
    "classifier.fit(feature_extractor(train_X), train_y)\n",
    "\n",
    "# model/final pipeline\n",
    "model = lambda X: classifier(feature_extractor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance of the model on the training set\n",
    "train_y_star = model(train_X)\n",
    "\n",
    "\"The performance on the training set is {:.2f}. This however, does not tell us much about the actual performance (generalisability).\".format(\n",
    "    accuracy_score(train_y, train_y_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels for the test set \n",
    "test_y_star = model(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Publishing best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test.copy().drop('img', axis = 1)\n",
    "submission['class'] = test_y_star\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Discussion\n",
    "...\n",
    "\n",
    "In summary we contributed the following: \n",
    "* \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
     }
    },
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     }
    },
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
